{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8344e0-0b22-495a-bba2-402284e4e8b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MISO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9062964d-4e9e-442b-bf01-bc7376326e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta\n",
    "import time\n",
    "import sys, os\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import wget\n",
    "import urllib\n",
    "import warnings\n",
    "import requests\n",
    "\n",
    "\n",
    "start_time = input('Enter the start time(YYYY-MM-DD): ')\n",
    "if len(start_time) < 1:\n",
    "    start_time = '2025-01-01'\n",
    "\n",
    "end_time = input('Enter the end time(YYYY-MM-DD): ')\n",
    "if len(end_time) < 1:\n",
    "    end_time = '2025-01-02'\n",
    "\n",
    "variable_name = input('Enter the Variable name: ')\n",
    "if len(variable_name) < 1:\n",
    "    variable_name = 'demand_forecast_hourly_regional_MISO'\n",
    "\n",
    "info_dict_MISO = {\n",
    "    'solar_forecast_hourly_MISO':{'variable_str':'_mom.xlsx','variable_date_str':'SOLAR HOURLY', \"Unique_variable_name_ISO\": \" \"},\n",
    "    'wind_forecast_hourly_MISO':{'variable_str':'_mom.xlsx','variable_date_str':'WIND HOURLY'},\n",
    "    'outage_forecast_daily_MISO':{'variable_str':'_mom.xlsx','variable_date_str':'OUTAGE'},\n",
    "    'demand_forecast_hourly_zone_MISO':{'variable_str':'df_al.xls','variable_date_str':'n/a'},\n",
    "    'demand_forecast_hourly_regional_MISO':{'variable_str':'rf_al.xls','variable_date_str':'n/a'},\n",
    "    'DA_LMP_MISO':{'variable_str':'da_exante_lmp.csv','variable_date_str':'n/a'},# Miso will change it to New API\n",
    "    'RT_LMP_MISO':{'variable_str':'5min_exante_lmp.xlsx','variable_date_str':'n/a'},# Miso will change it to New API\n",
    "    'DA_HUB_price_MISO': {'variable_str': 'da_pr.xls','variable_date_str':'n/a'},\n",
    "    'RT_HUB_price_MISO': {'variable_str': 'rt_pr.xls','variable_date_str':'n/a'},\n",
    "    }\n",
    "\n",
    "def date_range_list(start_date, end_date, days_num=1):\n",
    "    # Return generator for a list datetime.date objects (inclusive) between start_date and end_date (inclusive).\n",
    "    curr_date = start_date\n",
    "    while curr_date <= end_date:\n",
    "        yield curr_date \n",
    "        curr_date += timedelta(days=days_num)\n",
    "\n",
    "\n",
    "def miso_data_downloading(start_time, end_time, variable_name, info_dict_MISO):\n",
    "    start_time_list = start_time.split('-')\n",
    "    end_time_list = end_time.split('-')\n",
    "\n",
    "    d0 = date(int(start_time_list[0]), int(start_time_list[1]), int(start_time_list[2]))\n",
    "    d1 = date(int(end_time_list[0]), int(end_time_list[1]), int(end_time_list[2]))\n",
    "    data_array = []\n",
    "    date_list = date_range_list(d0, d1)\n",
    "\n",
    "    variable_str = info_dict_MISO[variable_name]['variable_str']\n",
    "    variable_date_str = info_dict_MISO[variable_name]['variable_date_str']\n",
    "    \n",
    "    ###\n",
    "    # variable_name=='DA_HUB_price_MISO' or variable_name=='RT_LMP_MISO'\n",
    "    ###\n",
    "    if variable_name=='DA_HUB_price_MISO' or variable_name=='RT_LMP_MISO':\n",
    "        for day in date_list:\n",
    "            date_list = str(day).split('-')\n",
    "            YYYYMMDD = date_list[0]+date_list[1]+date_list[2]\n",
    "            url = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD +'_' + variable_str\n",
    "\n",
    "            df_0 = pd.read_excel(url, index_col=None,na_values=['NA'], usecols=\"B\")\n",
    "            #print(df_0)\n",
    "            header_num = 0\n",
    "            header = 0\n",
    "            Header_str = 'NaN'\n",
    "            while Header_str != 'MISO System':\n",
    "                Header_str = str(df_0.iloc[header,0])\n",
    "                header = header_num\n",
    "                header_num = header_num + 1\n",
    "                if header > len(df_0['Unnamed: 1']):\n",
    "                    break\n",
    "            #print(header)\n",
    "            df = pd.read_excel(io = url,index_col= None , na_values=['NA'], header=header, nrows = 24)#, usecols=[\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\"]\n",
    "            print(df)\n",
    "            if str(day) == str(start_time):\n",
    "                sum_df = df\n",
    "            else:\n",
    "                sum_df = pd.concat([sum_df, df], ignore_index = True)\n",
    "\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # elif (variable_name == 'solar_forecast_hourly_MISO' or variable_name == 'wind_forecast_hourly_MISO' or variable_name == 'outage_forecast_daily_MISO')\n",
    "    ###\n",
    "    elif (variable_name == 'solar_forecast_hourly_MISO' or variable_name == 'wind_forecast_hourly_MISO' \n",
    "          or variable_name == 'outage_forecast_daily_MISO'):\n",
    "        i = 0\n",
    "        for day in date_list:\n",
    "            date_str = str(day).split('-')\n",
    "            YYYYMMDD = date_str[0]+date_str[1]+date_str[2]\n",
    "            current_date = day + timedelta(days = 1)\n",
    "            current_date_str = str(current_date).split('-')\n",
    "            \n",
    "            url_2 = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD + '_'+variable_str\n",
    "            print(YYYYMMDD)\n",
    "            header = 0\n",
    "            try:\n",
    "                df = pd.read_excel(url_2, index_col=None, sheet_name=variable_date_str,na_values=['NA'], usecols=\"A\")\n",
    "                header_num = 0\n",
    "                header = 0\n",
    "                Header_str = 'NaN'\n",
    "                while Header_str != 'DAY HE' and Header_str != 'Day HE':#or Header_str != 'Day HE'\n",
    "                    Header_str = str(df.iloc[header,0])\n",
    "                    header = header_num\n",
    "                    header_num = header_num + 1\n",
    "                    if header > len(df['Unnamed: 0']):\n",
    "                        break\n",
    "                df_mom = pd.read_excel(io = url_2,index_col= None , na_values=['NA'], header=header, usecols='A,B,C,D',#,D\n",
    "                                     sheet_name=variable_date_str,nrows = 48,engine=\"openpyxl\")\n",
    "                time_info = 'NaN'\n",
    "                time_info_1am = current_date_str[1] + '/'+ current_date_str[2] + '/' + current_date_str[0] + ' 1'\n",
    "                time_info_7am = current_date_str[1] + '/'+ current_date_str[2] + '/' + current_date_str[0] + ' 7'\n",
    "                \n",
    "                time_idx = 0\n",
    "                while time_info != time_info_1am:\n",
    "                    time_info = str(''.join(list(df_mom.iloc[time_idx,0])[2:15]))\n",
    "                    time_idx = time_idx + 1\n",
    "                    #if time_info = \n",
    "                    if time_idx > len(df_mom.iloc[:,0]):\n",
    "                        break\n",
    "                iloc_start = time_idx - 1\n",
    "                iloc_end = iloc_start+24\n",
    "                \n",
    "                \n",
    "                df_mom = df_mom.iloc[iloc_start:iloc_end,:]\n",
    "                data_array.append(1)\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_excel(url_1, index_col=None, sheet_name=variable_date_str,na_values=['NA'], usecols=\"A\")\n",
    "                    header_num = 0\n",
    "                    header = 0\n",
    "                    Header_str = 'NaN'\n",
    "                    while Header_str != 'DAY HE'and Header_str != 'Day HE':#or Header_str != 'Day HE'\n",
    "                        Header_str = str(df.iloc[header,0])\n",
    "                        header = header_num\n",
    "                        header_num = header_num + 1\n",
    "                        if header > len(df['Unnamed: 0']):\n",
    "                            break\n",
    "        \n",
    "                    df_mom = pd.read_excel(io = url_1,index_col= None , na_values=['NA'], header=header, usecols='A,B,C,D',#,D\n",
    "                                     sheet_name=variable_date_str,nrows = 168,engine=\"openpyxl\")\n",
    "                    time_info = 'NaN'\n",
    "                    time_info_1am = current_date_str[1] + '/'+ current_date_str[2] + '/' + current_date_str[0] + ' 1'\n",
    "                    time_info_7am = current_date_str[1] + '/'+ current_date_str[2] + '/' + current_date_str[0] + ' 7'\n",
    "        \n",
    "                    time_idx = 0\n",
    "                    while time_info != time_info_1am:\n",
    "                        time_info = str(''.join(list(df_mom.iloc[time_idx,0])[2:15]))\n",
    "                        time_idx = time_idx + 1\n",
    "                        if time_idx > len(df_mom.iloc[:,0]):\n",
    "                            break\n",
    "                    iloc_start = time_idx - 1\n",
    "                    iloc_end = iloc_start+24\n",
    "        \n",
    "                    df_mom = df_mom.iloc[iloc_start:iloc_end,:]\n",
    "                    data_array.append(2)\n",
    "            \n",
    "                except:\n",
    "                    intial_date = date(int(date_str[0]), int(date_str[1]), int(date_str[2]))\n",
    "                    day_i = 0\n",
    "                    #day_i = day_i + 1\n",
    "                    file_date = intial_date + timedelta(days = -day_i)\n",
    "                    file_date_str = str(file_date).split('-')\n",
    "                    YYYYMMDD_1 = file_date_str[0]+file_date_str[1]+file_date_str[2]\n",
    "                    url_try = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD_1 + variable_str\n",
    "        \n",
    "                    response = requests.get(url_try)\n",
    "                    while response.status_code != 200:\n",
    "                        file_date = intial_date + timedelta(days = -day_i)\n",
    "                        file_date_str = str(file_date).split('-')\n",
    "                        YYYYMMDD_1 = file_date_str[0]+file_date_str[1]+file_date_str[2]\n",
    "                        url_try = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD_1 + variable_str\n",
    "                        response = requests.get(url_try)\n",
    "                        day_i = day_i + 1\n",
    "                        print(\"Found previous: \", YYYYMMDD_1)\n",
    "                    #url_1_1 = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD + '_mom.xlsx'\n",
    "                    df = pd.read_excel(url_try, index_col=None, sheet_name='WIND HOURLY',na_values=['NA'], usecols=\"A\")\n",
    "                    header_num = 0\n",
    "                    header = 0\n",
    "                    Header_str = 'NaN'\n",
    "                    while Header_str != 'DAY HE'and Header_str != 'Day HE':#or Header_str != 'Day HE'\n",
    "                        Header_str = str(df.iloc[header,0])\n",
    "                        header = header_num\n",
    "                        header_num = header_num + 1\n",
    "                        if header > len(df['Unnamed: 0']):\n",
    "                            break\n",
    "        \n",
    "                    df_mom = pd.read_excel(io = url_try,index_col= None , na_values=['NA'], header=header, usecols='A,B,C,D',#,D\n",
    "                                     sheet_name='WIND HOURLY',nrows = 168,engine=\"openpyxl\")\n",
    "                    time_info = 'NaN'\n",
    "                    time_info_1am = current_date_str[1] + '/'+ current_date_str[2] + '/' + current_date_str[0] + ' 1'\n",
    "                    time_info_7am = current_date_str[1] + '/'+ current_date_str[2] + '/' + current_date_str[0] + ' 7'\n",
    "        \n",
    "                    time_idx = 0\n",
    "                    while time_info != time_info_1am:\n",
    "                        time_info = str(''.join(list(df_mom.iloc[time_idx,0])[2:15]))\n",
    "                        time_idx = time_idx + 1\n",
    "                        if time_idx > len(df_mom.iloc[:,0]):\n",
    "                            break\n",
    "                    iloc_start = time_idx - 1\n",
    "                    iloc_end = iloc_start+24\n",
    "        \n",
    "                    df_mom = df_mom.iloc[iloc_start:iloc_end,:]\n",
    "                    data_array.append(2)\n",
    "                if Header_str == 'Day HE':\n",
    "                    df_mom = df_mom.rename(columns={'Day HE': 'DAY HE', 'North': 'North','Central': 'Central'})\n",
    "            \n",
    "            if str(day) == str(start_time):\n",
    "                sum_df = df_mom\n",
    "            else:\n",
    "                sum_df = pd.concat([sum_df, df_mom], ignore_index = False)#, ignore_index = True\n",
    "            url_1 = url_2\n",
    "            i = i + 1\n",
    "    ###\n",
    "    # elif (variable_name == 'outage_forecast_daily_MISO')\n",
    "    ###\n",
    "    elif (variable_name == 'outage_forecast_daily_MISO'):\n",
    "        i = 0\n",
    "        for day in date_list:\n",
    "            date_str = str(day).split('-')\n",
    "            YYYYMMDD = date_str[0]+date_str[1]+date_str[2]\n",
    "            current_date = day + timedelta(days = 1)\n",
    "            current_date_str = str(current_date).split('-')\n",
    "            \n",
    "            url_2 = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD + '_' + variable_str\n",
    "            print(YYYYMMDD)\n",
    "            header = 0\n",
    "            try:\n",
    "                #df = pd.read_excel(url_2, index_col=None, sheet_name='OUTAGE',na_values=['NA'], usecols=\"C\")\n",
    "                #header_num = 0\n",
    "                header = 6\n",
    "                index_day = 2\n",
    "                df_outage = pd.read_excel(io = url_2,index_col= None , na_values=['NA'], header=header, usecols='A,B,C',#,D\n",
    "                                     sheet_name=variable_date_str,nrows = 16,engine=\"openpyxl\")\n",
    "                \n",
    "                sum_north = df_outage[df_outage[\"Unnamed: 0\"]==\"North\"].sum()\n",
    "                sum_central = df_outage[df_outage[\"Unnamed: 0\"]==\"Central\"].sum()\n",
    "                sum_south = df_outage[df_outage[\"Unnamed: 0\"]==\"South\"].sum()\n",
    "                sum_miso = df_outage[df_outage[\"Unnamed: 0\"]==\"MISO\"].sum()\n",
    "                df_sum_data_day =  pd.DataFrame({'Outage_North': [sum_north.iloc[index_day]],'Outage_Central': [sum_central.iloc[index_day]],\n",
    "                               'Outage_South': [sum_south.iloc[index_day]],'Outage_MISO': [sum_miso.iloc[index_day]]})\n",
    "                df_outage_sum = pd.DataFrame()\n",
    "                for i_hour in range(24):\n",
    "                    df_outage_sum = pd.concat([df_sum_data_day, df_outage_sum], ignore_index = False)\n",
    "                \n",
    "                data_array.append(1)\n",
    "            except:\n",
    "                try:\n",
    "                    #df = pd.read_excel(url_1, index_col=None, sheet_name='SOLAR HOURLY',na_values=['NA'], usecols=\"A,B,C,D\")\n",
    "                    header_num = 0\n",
    "                    header = 6\n",
    "                    \n",
    "                    df_outage = pd.read_excel(io = url_1,index_col= None , na_values=['NA'], header=header, usecols='A,B,C,D',#,D\n",
    "                                     sheet_name=variable_date_str,nrows = 16,engine=\"openpyxl\")\n",
    "                    index_day = 3\n",
    "                    sum_north = df_outage[df_outage[\"Unnamed: 0\"]==\"North\"].sum()\n",
    "                    sum_central = df_outage[df_outage[\"Unnamed: 0\"]==\"Central\"].sum()\n",
    "                    sum_south = df_outage[df_outage[\"Unnamed: 0\"]==\"South\"].sum()\n",
    "                    sum_miso = df_outage[df_outage[\"Unnamed: 0\"]==\"MISO\"].sum()\n",
    "                    df_sum_data_day =  pd.DataFrame({'Outage_North': [sum_north.iloc[index_day]],'Outage_Central': [sum_central.iloc[index_day]],\n",
    "                               'Outage_South': [sum_south.iloc[index_day]],'Outage_MISO': [sum_miso.iloc[index_day]]})\n",
    "                    df_outage_sum = pd.DataFrame()\n",
    "                    for i_hour in range(24):\n",
    "                        df_outage_sum = pd.concat([df_sum_data_day, df_outage_sum], ignore_index = False)\n",
    "                    data_array.append(2)\n",
    "            \n",
    "                except:\n",
    "                    print(\"Can not find: \", YYYYMMDD)\n",
    "                    intial_date = date(int(date_str[0]), int(date_str[1]), int(date_str[2]))\n",
    "                    day_i = 0\n",
    "                    #day_i = day_i + 1\n",
    "                    file_date = intial_date + timedelta(days = -day_i)\n",
    "                    file_date_str = str(file_date).split('-')\n",
    "                    YYYYMMDD_1 = file_date_str[0]+file_date_str[1]+file_date_str[2]\n",
    "                    url_try = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD_1+ '_'+variable_str\n",
    "        \n",
    "                    response = requests.get(url_try)\n",
    "                    while response.status_code != 200:\n",
    "                        file_date = intial_date + timedelta(days = -day_i)\n",
    "                        file_date_str = str(file_date).split('-')\n",
    "                        YYYYMMDD_1 = file_date_str[0]+file_date_str[1]+file_date_str[2]\n",
    "                        url_try = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD_1 + '_' + variable_str\n",
    "                        response = requests.get(url_try)\n",
    "                        day_i = day_i + 1\n",
    "                    #print(\"Found previous: \", YYYYMMDD_1)\n",
    "                    #url_1_1 = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD + '_mom.xlsx'\n",
    "                    #df = pd.read_excel(url_try, index_col=None, sheet_name='OUTAGE',na_values=['NA'], usecols=\"A,B,C,D,E,F,G,H,I\")\n",
    "                    header = 6\n",
    "                    \n",
    "                    df_outage = pd.read_excel(io = url_try,index_col= None , na_values=['NA'], header=header, usecols=\"A,B,C,D,E,F,G,H,I\",#,D\n",
    "                                     sheet_name=variable_date_str,nrows = 16,engine=\"openpyxl\")\n",
    "                    index_day = day_i+1\n",
    "                    print(\"!!!!!\",index_day)\n",
    "                    \n",
    "                    sum_north = df_outage[df_outage[\"Unnamed: 0\"]==\"North\"].sum()\n",
    "                    sum_central = df_outage[df_outage[\"Unnamed: 0\"]==\"Central\"].sum()\n",
    "                    sum_south = df_outage[df_outage[\"Unnamed: 0\"]==\"South\"].sum()\n",
    "                    sum_miso = df_outage[df_outage[\"Unnamed: 0\"]==\"MISO\"].sum()\n",
    "                    df_sum_data_day =  pd.DataFrame({'Outage_North': [sum_north.iloc[index_day]],'Outage_Central': [sum_central.iloc[index_day]],\n",
    "                               'Outage_South': [sum_south.iloc[index_day]],'Outage_MISO': [sum_miso.iloc[index_day]]})\n",
    "                    df_outage_sum = pd.DataFrame()\n",
    "                    for i_hour in range(24):\n",
    "                        df_outage_sum = pd.concat([df_sum_data_day, df_outage_sum], ignore_index = False)\n",
    "                    print(df_outage_sum)\n",
    "                data_array.append(2)\n",
    "                \n",
    "            \n",
    "            if str(day) == str(start_time):\n",
    "                sum_df = df_outage_sum\n",
    "            else:\n",
    "                sum_df = pd.concat([sum_df, df_outage_sum], ignore_index = False)#, ignore_index = True\n",
    "            url_1 = url_2\n",
    "            i = i + 1\n",
    "    ###\n",
    "    # elif (variable_name == 'demand_forecast_hourly_zone_MISO')\n",
    "    ###\n",
    "    elif variable_name == 'demand_forecast_hourly_zone_MISO':\n",
    "        for day in date_list:\n",
    "            date_list = str(day).split('-')\n",
    "            YYYYMMDD = date_list[0]+date_list[1]+date_list[2]\n",
    "            url = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD + '_' + variable_str\n",
    "        \n",
    "            df = pd.read_excel(io = url,index_col= None , na_values=['NA'], \n",
    "                               header=None, skiprows = [i for i in (range(32))], nrows = 24).drop([3,5,7,9,11,13,15], axis=1)#, usecols=[\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\"]\n",
    "            df.rename(columns={0: 'Date', 1: 'Hour', 2: 'Zone1', 4: 'Zone2_7', 6: 'Zone3_5', 8: 'Zone4', 10: 'Zone6', 12: 'Zone8_9', 14: 'MISO'}, inplace=True)#print(day)\n",
    "            #print(df)\n",
    "            if str(day) == str(start_time):\n",
    "                sum_df = df\n",
    "            else:\n",
    "                sum_df = pd.concat([sum_df, df], ignore_index = True)\n",
    "\n",
    "    ###\n",
    "    # elif (variable_name == 'demand_forecast_hourly_zone_MISO')\n",
    "    ###\n",
    "    elif (variable_name == 'demand_forecast_hourly_regional_MISO'):\n",
    "        for day in date_list:\n",
    "            date_list = str(day).split('-')\n",
    "            YYYYMMDD = date_list[0]+date_list[1]+date_list[2]\n",
    "            url = 'https://docs.misoenergy.org/marketreports/'+ YYYYMMDD + '_' + variable_str\n",
    "    \n",
    "            df = pd.read_excel(io = url,index_col= None , na_values=['NA'],# North - D; Central - F; MISO - J\n",
    "                               header=None, skiprows = [i for i in (range(34))], nrows = 24).drop([0, 4,6,8,10], axis=1)\n",
    "            df.rename(columns={1: 'Date', 2: 'Hour', 3: 'North', 5: 'Central', 7: 'South', 9: 'MISO'}, inplace=True)\n",
    "            #print(day)\n",
    "            if str(day) == str(start_time):\n",
    "                sum_df = df\n",
    "            else:\n",
    "                sum_df = pd.concat([sum_df, df], ignore_index = True)\n",
    "        \n",
    "    return sum_df\n",
    "\n",
    "\n",
    "sum_df = miso_data_downloading(start_time, end_time, variable_name, info_dict_MISO)\n",
    "print(sum_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473ed01-f8a7-484c-8db7-a1349fac3153",
   "metadata": {},
   "source": [
    "# PJM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4315a8-283e-4e4b-9903-a84356abde08",
   "metadata": {},
   "source": [
    "## 1. Downloading Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59349543-5957-4087-8105-fb34d180a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta,date\n",
    "import time\n",
    "import random\n",
    "\n",
    "start_time = input('Enter the start time(YYYY-MM-DD): ')\n",
    "if len(start_time) < 1:\n",
    "    start_time = '2024-01-01'\n",
    "\n",
    "end_time = input('Enter the end time(YYYY-MM-DD): ')\n",
    "if len(end_time) < 1:\n",
    "    end_time = '2025-10-01'\n",
    "\n",
    "variable_name = input('Enter the Variable name: ')\n",
    "if len(variable_name) < 1:\n",
    "    variable_name = 'demand_forecast_hourly_PJM'\n",
    "\n",
    "info_dict_PJM = {'solar_forecast_5min_PJM':{'variable_str':'five_min_solar_power_forecast','variable_date_str':'datetime_beginning_utc'},\n",
    "    'solar_forecast_hourly_PJM':{'variable_str':'hourly_solar_power_forecast','variable_date_str':'datetime_beginning_utc'},\n",
    "    'wind_forecast_5min_PJM':{'variable_str':'five_min_wind_power_forecast','variable_date_str':'datetime_beginning_utc'},\n",
    "    'wind_forecast_hourly_PJM':{'variable_str':'hourly_wind_power_forecast','variable_date_str':'datetime_beginning_utc'},\n",
    "    'outage_forecast_daily_PJM':{'variable_str':'frcstd_gen_outages','variable_date_str':'forecast_date'},\n",
    "    'demand_forecast_hourly_PJM':{'variable_str':'load_frcstd_hist','variable_date_str':'forecast_hour_beginning_utc'},\n",
    "    'DA_LMP_PJM':{'variable_str':'da_hrl_lmps','variable_date_str':'datetime_beginning_utc'},\n",
    "    'RT_LMP_PJM':{'variable_str':'rt_fivemin_mnt_lmps','variable_date_str':'datetime_beginning_utc'},\n",
    "    'demand_real_hourly_PJM':{'variable_str':'frcstd_gen_outages','variable_date_str':'forecast_date'},\n",
    "    }\n",
    "def date_range_list(start_date, end_date, days_num=1):\n",
    "    # Return generator for a list datetime.date objects (inclusive) between start_date and end_date (inclusive).\n",
    "    curr_date = start_date\n",
    "    while curr_date <= end_date:\n",
    "        yield curr_date \n",
    "        curr_date += timedelta(days=days_num)\n",
    "    if curr_date != end_date:\n",
    "        yield end_date\n",
    "\n",
    "def pjm_data_downloading(start_time, end_time, variable_name, info_dict_PJM):\n",
    "    variable_str = info_dict_PJM[variable_name]['variable_str']\n",
    "    variable_date_str = info_dict_PJM[variable_name]['variable_date_str']\n",
    "    PJM_no_hist_variable_list = ['solar_forecast_5min_PJM','solar_forecast_hourly_PJM','wind_forecast_5min_PJM','wind_forecast_hourly_PJM']\n",
    "    \n",
    "    hdr ={\n",
    "            # Request headers\n",
    "            'Cache-Control': 'no-cache',\n",
    "            'Ocp-Apim-Subscription-Key': '336423db3cd243a3841d772330f22cc7',\n",
    "            }\n",
    "    urls = []\n",
    "    df_results = pd.DataFrame()\n",
    "    \n",
    "    ####################################\n",
    "    if variable_name in PJM_no_hist_variable_list:\n",
    "        print(\"Note: PJM does not store renewable forecast data beyond 30 days, the function will turn to download last 30 days' data\")\n",
    "        current_date = datetime.now()# Get the current date\n",
    "        formatted_current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        formatted_30_days_ago_date = (datetime.now()-timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
    "        if (variable_name == 'solar_forecast_5min_PJM' or variable_name == 'wind_forecast_5min_PJM'):\n",
    "        ## 5 min level data beyond the capacity of single PJM API call, turn it to mutiple API call\n",
    "            try:\n",
    "                for iter_days in range(9):\n",
    "                    start_time_iter = (current_date-timedelta(days=iter_days*3+3)).strftime(\"%Y-%m-%d\")\n",
    "                    end_time_iter = (current_date-timedelta(days=iter_days*3)).strftime(\"%Y-%m-%d\")\n",
    "                    url = (\"https://api.pjm.com/api/v1/\" + variable_str +\n",
    "                           \"?rowCount=50000&order=Asc&startRow=1&\" + variable_date_str + \n",
    "                           \"=\" + start_time_iter + \"%2005:00%20to%20\" + end_time_iter + \"%2004:00\")\n",
    "                    urls.append(url)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            try:\n",
    "                start_time_iter = current_date\n",
    "                url = (\"https://api.pjm.com/api/v1/\" + variable_str +\n",
    "                       \"?rowCount=50000&order=Asc&startRow=1&\" + variable_date_str + \n",
    "                       \"=\" + formatted_30_days_ago_date + \"%2005:00%20to%20\" + formatted_current_date + \"%2004:00\")\n",
    "                urls.append(url)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    ####################################       \n",
    "    elif variable_name == 'outage_forecast_daily_PJM':\n",
    "        try:\n",
    "            url = (\"https://api.pjm.com/api/v1/\" + variable_str +\n",
    "                   \"?rowCount=50000&order=Asc&startRow=1&\" + variable_date_str + \n",
    "                   \"=\" + start_time)\n",
    "            \n",
    "            urls.append(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    ####################################\n",
    "    elif variable_name == 'DA_LMP_PJM'or variable_name == 'RT_LMP_PJM':  \n",
    "        try:\n",
    "            start_time_list = start_time.split('-')\n",
    "            end_time_list = end_time.split('-')\n",
    "            d0 = date(int(start_time_list[0]), int(start_time_list[1]), int(start_time_list[2]))\n",
    "            d1 = date(int(end_time_list[0]), int(end_time_list[1]), int(end_time_list[2]))\n",
    "            \n",
    "            if variable_name == 'DA_LMP_PJM':#increase download threadhold\n",
    "                days_num = 300\n",
    "            else:\n",
    "                days_num = 150\n",
    "            \n",
    "            date_var_list = date_range_list(d0, d1, days_num= days_num)\n",
    "            urls = []\n",
    "            iter_num = 0\n",
    "            for i_date in date_var_list:\n",
    "                #print(str(i_date),iter_num)\n",
    "                if iter_num == 0:\n",
    "                    start_d = str(i_date)\n",
    "                elif iter_num != 0:\n",
    "                    end_d = str(i_date)\n",
    "                    if variable_name == 'DA_LMP_PJM':\n",
    "                        url = (\"https://api.pjm.com/api/v1/\" + variable_str +\n",
    "                                   \"?rowCount=50000&order=Asc&startRow=1&\" + variable_date_str + \n",
    "                                   \"=\" + start_d + \"%2005:00%20to%20\" + end_d + \"%2004:00\"+\"&pnode_id=1\")\n",
    "                    elif variable_name == 'RT_LMP_PJM':\n",
    "                        url = (\"https://api.pjm.com/api/v1/\" + variable_str +\n",
    "                                   \"?rowCount=50000&order=Asc&startRow=1&\" + variable_date_str + \n",
    "                                   \"=\" + start_d + \"%2005:00%20to%20\" + end_d + \"%2004:55\"+\"&pnode_id=1\")\n",
    "                    urls.append(url)\n",
    "                    start_d = str(i_date)\n",
    "                iter_num = iter_num + 1\n",
    "            #urls.append(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    ####################################\n",
    "    else:  \n",
    "        try:\n",
    "            start_time_list = start_time.split('-')\n",
    "            end_time_list = end_time.split('-')\n",
    "            d0 = date(int(start_time_list[0]), int(start_time_list[1]), int(start_time_list[2]))\n",
    "            d1 = date(int(end_time_list[0]), int(end_time_list[1]), int(end_time_list[2]))\n",
    "            if variable_name == 'DA_LMP_PJM':\n",
    "                days_num \n",
    "            date_var_list = date_range_list(d0, d1, days_num=7)\n",
    "            urls = []\n",
    "            iter_num = 0\n",
    "            for i_date in date_var_list:\n",
    "                #print(str(i_date),iter_num)\n",
    "                if iter_num == 0:\n",
    "                    start_d = str(i_date)\n",
    "                elif iter_num != 0:\n",
    "                    end_d = str(i_date)\n",
    "                    url = (\"https://api.pjm.com/api/v1/\" + variable_str +\n",
    "                               \"?rowCount=50000&order=Asc&startRow=1&\" + variable_date_str + \n",
    "                               \"=\" + start_d + \"%2005:00%20to%20\" + end_d + \"%2004:00\")\n",
    "                    print(url)\n",
    "                    urls.append(url)\n",
    "                    start_d = str(i_date)\n",
    "                iter_num = iter_num + 1\n",
    "            \n",
    "            #urls.append(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    ####################################\n",
    "    for i_url, url in enumerate(urls):\n",
    "        #time.sleep(random.randint(1, 5))\n",
    "        req = urllib.request.Request(url, headers=hdr)\n",
    "        req.get_method = lambda: 'GET'\n",
    "        response = urllib.request.urlopen(req)\n",
    "        print(response.getcode())\n",
    "        results = response.read()\n",
    "        json_data = json.loads(results.decode('utf-8'))\n",
    "        print(url)\n",
    "        df_results_1 = pd.DataFrame(json_data['items'])\n",
    "        print(df_results_1)\n",
    "        if i_url % 4 == 0 and i_url!=0:#the whole loop used to reset the PJM api call\n",
    "            time.sleep(random.randint(60, 62)) #take a sleep to make it like human bechavior\n",
    "            url_random = \"https://api.pjm.com/api/v1/load_frcstd_hist?rowCount=50000&order=Asc&startRow=1\"\n",
    "            req = urllib.request.Request(url_random, headers=hdr)#take a random api call to reset the count\n",
    "            req.get_method = lambda: 'GET'\n",
    "            response = urllib.request.urlopen(req)\n",
    "            print('Reset the api count')\n",
    "        df_results = pd.concat([df_results,df_results_1], ignore_index=True)\n",
    "    #print(df_results)\n",
    "    \n",
    "    return df_results\n",
    "df_results = pjm_data_downloading(start_time, end_time, variable_name, info_dict_PJM)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417d473-d40a-4330-8c90-92d29063571b",
   "metadata": {},
   "source": [
    "## 2. Processing Example\n",
    "Aim to get the best availible demand forecast and insert real demand data if the forecast data are missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b7716c-3946-45c6-bce9-a0d0b7117877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                evaluated_at_utc     evaluated_at_ept  \\\n",
      "forecast_hour_beginning_utc                                             \n",
      "2024-01-01T05:00:00          2023-12-31T14:45:00  2023-12-31T09:45:00   \n",
      "2024-01-01T06:00:00          2023-12-31T14:45:00  2023-12-31T09:45:00   \n",
      "2024-01-01T07:00:00          2023-12-31T14:45:00  2023-12-31T09:45:00   \n",
      "2024-01-01T08:00:00          2023-12-31T14:45:00  2023-12-31T09:45:00   \n",
      "2024-01-01T09:00:00          2023-12-31T14:45:00  2023-12-31T09:45:00   \n",
      "...                                          ...                  ...   \n",
      "2025-09-30T19:00:00          2025-09-29T13:45:00  2025-09-29T09:45:00   \n",
      "2025-09-30T20:00:00          2025-09-29T13:45:00  2025-09-29T09:45:00   \n",
      "2025-09-30T21:00:00          2025-09-29T13:45:00  2025-09-29T09:45:00   \n",
      "2025-09-30T22:00:00          2025-09-29T13:45:00  2025-09-29T09:45:00   \n",
      "2025-09-30T23:00:00          2025-09-29T13:45:00  2025-09-29T09:45:00   \n",
      "\n",
      "                            forecast_hour_beginning_ept  AEP_mw  EKPC_mw  \\\n",
      "forecast_hour_beginning_utc                                                \n",
      "2024-01-01T05:00:00                 2024-01-01T00:00:00   14165     1885   \n",
      "2024-01-01T06:00:00                 2024-01-01T01:00:00   13929     1902   \n",
      "2024-01-01T07:00:00                 2024-01-01T02:00:00   13782     1901   \n",
      "2024-01-01T08:00:00                 2024-01-01T03:00:00   13803     1911   \n",
      "2024-01-01T09:00:00                 2024-01-01T04:00:00   13878     1923   \n",
      "...                                                 ...     ...      ...   \n",
      "2025-09-30T19:00:00                 2025-09-30T15:00:00   19061     2024   \n",
      "2025-09-30T20:00:00                 2025-09-30T16:00:00   19343     2119   \n",
      "2025-09-30T21:00:00                 2025-09-30T17:00:00   19448     2132   \n",
      "2025-09-30T22:00:00                 2025-09-30T18:00:00   19128     2055   \n",
      "2025-09-30T23:00:00                 2025-09-30T19:00:00   18641     1949   \n",
      "\n",
      "                             ATSI_mw  DEOK_mw  APS_mw  RTO_mw  DAY_mw  DUQ_mw  \\\n",
      "forecast_hour_beginning_utc                                                     \n",
      "2024-01-01T05:00:00           6595.0     2732  5335.0   84060  1747.0    1369   \n",
      "2024-01-01T06:00:00           6462.0     2676  5222.0   82132  1705.0    1338   \n",
      "2024-01-01T07:00:00           6355.0     2650  5175.0   81097  1683.0    1317   \n",
      "2024-01-01T08:00:00           6329.0     2638  5183.0   80822  1674.0    1306   \n",
      "2024-01-01T09:00:00           6352.0     2655  5221.0   81269  1680.0    1309   \n",
      "...                              ...      ...     ...     ...     ...     ...   \n",
      "2025-09-30T19:00:00           9018.0     4018  6443.0  109590  2624.0    1875   \n",
      "2025-09-30T20:00:00           9112.0     4077  6546.0  111528  2672.0    1910   \n",
      "2025-09-30T21:00:00           9089.0     4085  6582.0  111993  2687.0    1889   \n",
      "2025-09-30T22:00:00           8912.0     3975  6451.0  110244  2618.0    1823   \n",
      "2025-09-30T23:00:00           8691.0     3811  6335.0  107653  2509.0    1762   \n",
      "\n",
      "                             DOM_mw  COMED_mw  MIDATL_mw  \n",
      "forecast_hour_beginning_utc                               \n",
      "2024-01-01T05:00:00           13459      9580    27193.0  \n",
      "2024-01-01T06:00:00           13287      9257    26354.0  \n",
      "2024-01-01T07:00:00           13262      9044    25928.0  \n",
      "2024-01-01T08:00:00           13302      8916    25760.0  \n",
      "2024-01-01T09:00:00           13477      8857    25917.0  \n",
      "...                             ...       ...        ...  \n",
      "2025-09-30T19:00:00           15719     14528    34280.0  \n",
      "2025-09-30T20:00:00           15963     14626    35160.0  \n",
      "2025-09-30T21:00:00           15997     14550    35534.0  \n",
      "2025-09-30T22:00:00           16030     14111    35141.0  \n",
      "2025-09-30T23:00:00           15843     13468    34644.0  \n",
      "\n",
      "[15331 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta,date\n",
    "import urllib.request, json\n",
    "import numpy as np\n",
    "\n",
    "df_results = pd.read_csv(\"H:/BatteryStart/Dashboard/DATA/Griddata/demand_forecast_hourly_PJM.csv\")\n",
    "# the raw data\n",
    "def dates_bwn_two_dates(start_date, end_date):\n",
    "    '''\n",
    "    Generate dates between two given date\n",
    "    '''\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "def filter_demand_forecast(sdate, edate, df_results):\n",
    "    '''\n",
    "    filter the historal demand forecast to get the latest availible forecast before the bidding start\n",
    "    \n",
    "    sdate: start date in date format, for example, date(2024,1,1)\n",
    "    edate: end date in date format, for example, date(2025,10,1)\n",
    "    df_results: pandas dataframe \n",
    "    \n",
    "    '''\n",
    "    df_results['date'] = pd.to_datetime(df_results[\"forecast_hour_beginning_utc\"]).dt.date\n",
    "    date_timestamp_list = []\n",
    "    df_filtered = pd.DataFrame()\n",
    "    #print(dates_bwn_twodates(sdate,edate))\n",
    "    date_list = pd.date_range(sdate,edate-timedelta(days=1),freq='d')\n",
    "    for i_date in date_list:\n",
    "        i_date_yesterday = i_date-timedelta(days=1)\n",
    "        #print(i_date_yesterday)\n",
    "        i_date_str = str(i_date.date())\n",
    "        i_date_yesterday_str = str(i_date_yesterday.year) +'-' + str(i_date_yesterday.month) +'-' + str(i_date_yesterday.day) + \" 14:45:00\"\n",
    "        #print(i_date_yesterday_str)\n",
    "        target_time_date = datetime.strptime(i_date_str, \"%Y-%m-%d\")\n",
    "        target_time_yesterday_date = datetime.strptime(i_date_yesterday_str,\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        df_i_date = df_results[pd.to_datetime(df_results['date']) == target_time_date]\n",
    "        df_i_date = df_i_date[pd.to_datetime(df_i_date['evaluated_at_utc'])<=target_time_yesterday_date]\n",
    "        \n",
    "        df_filtered = pd.concat([df_filtered,df_i_date], axis = 0)\n",
    "        i_date_timestamp = df_i_date['evaluated_at_utc'].max()\n",
    "        #print(i_date_timestamp)\n",
    "        date_timestamp_list.append(i_date_timestamp)\n",
    "    df_rawdata_without_real_demand = df_filtered[df_filtered['evaluated_at_utc'].isin(date_timestamp_list)]\n",
    "    \n",
    "    return df_rawdata_without_real_demand\n",
    "\n",
    "def find_missing_demand_forecast(df_rawdata_without_real_demand):\n",
    "    '''\n",
    "    check which hour are missing from the demand forecast\n",
    "    \n",
    "    df_rawdata_without_real_demand(pandas DataFrame): the raw data with missing data as N/A or missing\n",
    "    '''\n",
    "    df_filtered = df_rawdata_without_real_demand[df_rawdata_without_real_demand['forecast_area']=='AEP'].sort_values(by=['forecast_hour_beginning_utc']).drop_duplicates(subset='forecast_hour_beginning_utc', keep='first')\n",
    "    \n",
    "    ## Find the missing hour in the dataset\n",
    "    df_AEP = df_rawdata_without_real_demand[df_rawdata_without_real_demand['forecast_area']=='AEP']\n",
    "    full_range = pd.date_range(start=pd.DatetimeIndex(df_AEP['forecast_hour_beginning_utc']).min(), end=pd.DatetimeIndex(df_AEP['forecast_hour_beginning_utc']).max(), freq=\"h\")\n",
    "    missing_hours = full_range.difference(pd.DatetimeIndex(df_AEP['forecast_hour_beginning_utc']))\n",
    "    #print(missing_hours)\n",
    "    date_str_pjm_api_call_list = []\n",
    "    #print(pd.to_missing_hours[:].date())\n",
    "    \n",
    "    ## Generate the continue date str for minized API call\n",
    "    count = 0\n",
    "    for i_time in missing_hours:\n",
    "        if count == 0:\n",
    "            start_date = i_time\n",
    "            intial_start_date = i_time\n",
    "            intial_end_date = i_time\n",
    "        \n",
    "        end_date = i_time\n",
    "        time_diff = end_date - start_date\n",
    "        #print((time_diff))\n",
    "        if time_diff > timedelta(hours=1):\n",
    "            date_str_pjm_api_call = str(intial_start_date.date())+\"%20\"+str(intial_start_date.time())+\"%20to%20\"+ str(start_date.date()) +\"%20\"+str(start_date.time())\n",
    "            intial_start_date = i_time\n",
    "            #print(date_str_pjm_api_call)\n",
    "            date_str_pjm_api_call_list.append(date_str_pjm_api_call)\n",
    "        start_date = i_time\n",
    "        if count == len(missing_hours)-1:\n",
    "            date_str_pjm_api_call = str(intial_start_date.date())+\"%20\"+str(intial_start_date.time())+\"%20to%20\"+ str(start_date.date()) +\"%20\"+str(start_date.time())\n",
    "            intial_start_date = i_time\n",
    "            #print(date_str_pjm_api_call)\n",
    "            date_str_pjm_api_call_list.append(date_str_pjm_api_call)\n",
    "        \n",
    "        count = count+1\n",
    "    return date_str_pjm_api_call_list\n",
    "\n",
    "def call_real_demand_fill_miss_demand_forecast(date_str_pjm_api_call_list):\n",
    "    '''\n",
    "    date_str_pjm_api_call_list(list): list that include date string for API call to fill missing demand forecast data. \n",
    "                                It looks like ['2024-01-21%2005:00:00%20to%202024-01-21%2012:00:00', '2024-03-10%2005:00:00%20to%202024-03-10%2011:00:00', '2024-06-29%2004:00:00%20to%202024-06-29%2011:00:00', '2025-03-09%2005:00:00%20to%202025-03-09%2011:00:00']\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    df_miss_data = pd.DataFrame()\n",
    "    df_with_real_data_as_missing = pd.DataFrame()\n",
    "    for i_call in date_str_pjm_api_call_list:\n",
    "        try:\n",
    "            url = \"https://api.pjm.com/api/v1/hrl_load_estimated?rowCount=50000&startRow=1&datetime_beginning_utc=\" + i_call\n",
    "            #2024-01-21%2005:00:00%20to%202024-01-21%2012:00:00\"\n",
    "        \n",
    "            hdr ={\n",
    "            # Request headers\n",
    "            'Cache-Control': 'no-cache',\n",
    "            'Ocp-Apim-Subscription-Key': '336423db3cd243a3841d772330f22cc7',\n",
    "            }\n",
    "            req = urllib.request.Request(url, headers=hdr)\n",
    "            req.get_method = lambda: 'GET'\n",
    "            response = urllib.request.urlopen(req)\n",
    "            #print(response.getcode())\n",
    "            results = response.read()\n",
    "            json_data = json.loads(results.decode('utf-8'))\n",
    "            #print(url)\n",
    "            df_results_1 = pd.DataFrame(json_data['items'])\n",
    "            #print(df_results_1)\n",
    "            \n",
    "            df_temp = df_results_1.rename(columns={\"datetime_beginning_utc\": \"forecast_hour_beginning_utc\", \n",
    "                                                   \"datetime_beginning_ept\": \"forecast_hour_beginning_ept\",\n",
    "                                                   \"load_area\": \"forecast_area\",\n",
    "                                                   \"estimated_load_hourly\": \"forecast_load_mw\",\n",
    "                                                  })#.loc()\n",
    "            \n",
    "            df_temp['evaluated_at_utc'] =  df_temp['forecast_hour_beginning_utc'] \n",
    "            df_temp['evaluated_at_ept'] =  df_temp['forecast_hour_beginning_ept']\n",
    "            df_temp= df_temp.drop(['datetime_ending_utc','datetime_ending_ept'], axis=1)\n",
    "            df_temp_groupsum = df_temp.groupby(['forecast_hour_beginning_utc','forecast_hour_beginning_ept']).sum().reset_index()\n",
    "            df_temp_groupsum['forecast_area'] = 'RTO'\n",
    "            #print(df_temp_groupsum)\n",
    "            \n",
    "            df_temp_0 = pd.concat([df_temp_groupsum['forecast_hour_beginning_utc'].rename(\"evaluated_at_utc\"),#.loc['forecast_hour_beginning_utc'],\n",
    "                                   df_temp_groupsum['forecast_hour_beginning_ept'].rename(\"evaluated_at_ept\"),\n",
    "                                   #df_temp_groupsum['forecast_hour_beginning_utc'],\n",
    "                                   df_temp_groupsum['forecast_hour_beginning_utc'],\n",
    "                                   df_temp_groupsum['forecast_hour_beginning_ept'], \n",
    "                                   df_temp_groupsum['forecast_area'],\n",
    "                                   df_temp_groupsum['forecast_load_mw']], axis=1)\n",
    "    \n",
    "            df_temp = pd.concat([df_temp,df_temp_0], axis = 0)\n",
    "            df_miss_data = pd.concat([df_miss_data,df_temp], axis = 0)\n",
    "            #print(df_miss_data)\n",
    "            #df_results_1.to_csv('H:/BatteryStart/Dashboard/DATA/Griddata/real_demand.csv')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return df_miss_data\n",
    "\n",
    "def download_demand_forecast_with_missing_data_filling(sdate, edate, df_results):\n",
    "    '''\n",
    "    make the oraginal dataframe to \n",
    "    \n",
    "    sdate: start date in date format, for example, date(2024,1,1)\n",
    "    edate: end date in date format, for example, date(2025,10,1)\n",
    "    df_results: pandas dataframe, \n",
    "    \n",
    "    '''\n",
    "    area_drop_list_for_real_demand = ['PJME', 'PJMW', 'DAYTON', 'FE']\n",
    "    \n",
    "    df_rawdata_without_real_demand = filter_demand_forecast(sdate, edate, df_results)\n",
    "    date_str_pjm_api_call_list = find_missing_demand_forecast(df_rawdata_without_real_demand)\n",
    "    df_miss_data = call_real_demand_fill_miss_demand_forecast(date_str_pjm_api_call_list)\n",
    "\n",
    "    df_with_real_data_as_missing = pd.concat([df_rawdata_without_real_demand.drop_duplicates(subset=['forecast_hour_beginning_utc','forecast_area'], \n",
    "                                                                         keep='first'),df_miss_data], axis = 0).sort_values(by=['forecast_hour_beginning_utc'])\n",
    "    df_with_real_data_as_missing = df_with_real_data_as_missing[~df_with_real_data_as_missing['forecast_area'].isin(area_drop_list_for_real_demand)]\n",
    "    #print('df_with_real_data_as_missing:',df_with_real_data_as_missing)\n",
    "    \n",
    "    unique_area = df_with_real_data_as_missing['forecast_area'].unique()\n",
    "    \n",
    "    count_area = 0\n",
    "    for i_area in unique_area:    \n",
    "        \n",
    "        df_i_area = df_with_real_data_as_missing[df_with_real_data_as_missing['forecast_area']==i_area]\n",
    "        \n",
    "        if count_area!=0:\n",
    "            df_i_area = df_i_area.drop(['forecast_area','date','evaluated_at_utc','evaluated_at_ept','forecast_hour_beginning_ept'], axis=1).rename(columns = {'forecast_load_mw':(i_area+'_mw')})#.set_index('forecast_hour_beginning_utc', inplace=True)    \n",
    "            df_i_area.set_index('forecast_hour_beginning_utc', inplace=True)\n",
    "            #df_forecast_load_area = pd.merge(df_forecast_load_area,df_i_area,on='forecast_hour_beginning_utc')\n",
    "            #df_forecast_load_area = df_forecast_load_area.join(df_i_area,on = 'forecast_hour_beginning_utc')#how='outer'\n",
    "            df_forecast_load_area = pd.concat([df_forecast_load_area,df_i_area],axis=1)\n",
    "        else:\n",
    "            df_i_area = df_i_area.drop(['forecast_area','date'], axis=1).rename(columns = {'forecast_load_mw':(i_area+'_mw')})\n",
    "            df_i_area.set_index('forecast_hour_beginning_utc', inplace=True)\n",
    "            df_forecast_load_area = df_i_area\n",
    "            \n",
    "        #print(df_i_area)\n",
    "        count_area = count_area + 1\n",
    "    \n",
    "    df_forecast_load_area.fillna(0, inplace=True)## replace the data which still miss as 0\n",
    "    \n",
    "    return df_forecast_load_area\n",
    "\n",
    "### example usage\n",
    "df_results = pd.read_csv(\"H:/BatteryStart/Dashboard/DATA/Griddata/demand_forecast_hourly_PJM.csv\")\n",
    "sdate = date(2024,1,1)\n",
    "edate = date(2025,10,1)\n",
    "\n",
    "df_forecast_load_area = download_demand_forecast_with_missing_data_filling(sdate, edate, df_results)\n",
    "print(df_forecast_load_area)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd296f35-9549-40c7-83d1-94ac280f1214",
   "metadata": {},
   "source": [
    "# National Weather Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd420550-19e8-46ec-b31f-9618fb6fa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import http.client\n",
    "from urllib.parse import urlparse\n",
    "variable_name = input('Enter the Variable Name (string): ')\n",
    "if len(variable_name) < 1:\n",
    "    variable_name = 'Temp'\n",
    "\n",
    "year_str = input('Enter the year (string): ')\n",
    "if len(year_str) < 1:\n",
    "    year_str = '2024'\n",
    "\n",
    "info_dict_NWS = {'Temp':{'variable_str':'temp','file_str': \"YEUZ98_KWBN_\"},\n",
    "    'DewPoint':{'variable_str':'td','file_str': \"YFUZ98_KWBN_\"},\n",
    "    'WindDirection':{'variable_str':'wdir','file_str': \"YBUZ98_KWBN_\"},\n",
    "    'WindSpeed':{'variable_str':'wspd','file_str': \"YCUZ98_KWBN_\"},\n",
    "    'SkyCover':{'variable_str':'sky','file_str': \"YAUZ98_KWBN_\"},\n",
    "    'RelativeHumidity':{'variable_str':'rhm','file_str': \"YRUZ98_KWBN_\"},\n",
    "    'ApparentTemp':{'variable_str':'apt','file_str': \"YTUZ98_KWBN_\"},\n",
    "    }\n",
    "\n",
    "#variable_str = info_dict_NWS[variable_name]['variable_str']\n",
    "#path_str = \"https://noaa-ndfd-pds.s3.amazonaws.com/wmo/\"+ variable_str + \"/\"\n",
    "#file_str = info_dict_NWS[variable_name]['file_str']\n",
    "\n",
    "def check_link_exists(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        conn = http.client.HTTPSConnection(parsed_url.netloc)\n",
    "        conn.request(\"HEAD\", parsed_url.path)\n",
    "        response = conn.getresponse()\n",
    "        return response.status == 200\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_file_name_previous(path_str, file_str,time_str, year_str, month_str, day_str, hour_int):#Used if the current hour's file is not exist\n",
    "    file_hour_index = 0\n",
    "    mins = 0\n",
    "    result = False\n",
    "    hour_delta = 1\n",
    "    url_index = (path_str + time_str + file_str + year_str \n",
    "                     + month_str + day_str + str(hour_int-hour_delta))\n",
    "    while result != True:\n",
    "        if mins>= 60:\n",
    "            mins = 0\n",
    "            hour_delta = hour_delta + 1\n",
    "            hour_idx = hour_int-hour_delta\n",
    "            url_index = (path_str + time_str + file_str + year_str\n",
    "                     + month_str + day_str + str(hour_idx))\n",
    "            #print(\"error\")\n",
    "            #break\n",
    "        if mins < 10:\n",
    "            mins_str = \"0\"+ str(mins)\n",
    "        else:\n",
    "            mins_str = str(mins)\n",
    "        url = url_index + mins_str\n",
    "        result = check_link_exists(url)\n",
    "        mins = mins + 1\n",
    "    file_exist_url = url_index + mins_str#str(mins-1)\n",
    "    return file_exist_url, hour_delta\n",
    "\n",
    "def check_file_name(path_str, file_str, time_str, year_str, month_str, day_str, hour_int):\n",
    "    file_hour_index = 0\n",
    "    mins = 0\n",
    "    result = False\n",
    "    url_index = (path_str + time_str + file_str + year_str \n",
    "                     + month_str + day_str + str(hour_int))\n",
    "    \n",
    "    while result != True:\n",
    "        if mins>= 60:\n",
    "            print(\"error:current hour's weather not availible\")\n",
    "            url_previous, file_hour_index = check_file_name_previous(path_str, file_str, time_str, year_str, month_str, day_str, hour_int)\n",
    "            #url_previous \n",
    "            file_exist_url = url_previous #url_index + str(mins-1)\n",
    "            return file_exist_url, file_hour_index\n",
    "            break\n",
    "        if mins < 10:\n",
    "            mins_str = \"0\"+ str(mins)\n",
    "        else:\n",
    "            mins_str = str(mins)\n",
    "        url = url_index + mins_str\n",
    "        result = check_link_exists(url)\n",
    "        mins = mins + 1\n",
    "    file_exist_url = url_index + mins_str#str(mins-1)\n",
    "    return file_exist_url, file_hour_index\n",
    "#print(check_file_name(url_index))\n",
    "\n",
    "\n",
    "def download_links_list_generation(info_dict,variable_name_str,hour_int,year_str):\n",
    "    \n",
    "    '''\n",
    "    info_dict: dict for the variable infomation list for \n",
    "    variable_name_str: which variable are downloading ('Temp','DewPoint','WindDirection','WindSpeed','SkyCover','RelativeHumidity','ApparentTemp')\n",
    "    hour_int: which hour as the reference for the forcast scan\n",
    "    year_str: which year you want downloading\n",
    "    '''\n",
    "    variable_str = info_dict[variable_name_str]['variable_str']\n",
    "    path_str = \"https://noaa-ndfd-pds.s3.amazonaws.com/wmo/\"+ variable_str + \"/\"\n",
    "    file_str = info_dict[variable_name_str]['file_str']\n",
    "    url_list = []\n",
    "    days_num_month = np.array([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "    \n",
    "    for i_month in range(12):\n",
    "        for i_day in range(days_num_month[i_month]):\n",
    "            if i_month <9:\n",
    "                month_str = '0'+str(i_month+1)\n",
    "            else:\n",
    "                month_str = str(i_month+1)\n",
    "            if i_day <9:\n",
    "                day_str = '0'+str(i_day+1)\n",
    "            else:\n",
    "                day_str = str(i_day+1)\n",
    "            time_str = year_str+'/'+ month_str +'/'+day_str+'/'\n",
    "            #url_index = (\"https://noaa-ndfd-pds.s3.amazonaws.com/wmo/temp/\" + time_str + \"YEUZ98_KWBN_\"+ year_str \n",
    "            #             + month_str + day_str + \"20\")\n",
    "            result, file_hour_index = check_file_name(path_str, file_str,time_str, year_str, month_str, day_str, hour_int)#url_index\n",
    "            print(result,file_hour_index)\n",
    "            url_list.append(result)\n",
    "            #print(check_file_name(url_index))\n",
    "    return url_list\n",
    "\n",
    "hour_int = 20\n",
    "url_list = download_links_list_generation(info_dict_NWS,variable_name,hour_int,year_str)\n",
    "print(url_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
